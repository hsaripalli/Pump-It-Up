{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsaripalli/Pump-It-Up/blob/main/model/Pump_it_up_Optuna_Tuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW4Sv_Ro8z3f"
      },
      "source": [
        "# **Notes**\n",
        "\n",
        "**Hyperparamter tuned using optuna library. Best accuracy of 82.4% on submission on submission.** \n",
        "\n",
        "Load Data:\n",
        "\n",
        "- Loaded and combined train and test csv files\n",
        "- Dropped columns that obviously did not have any significance (mostly 0s or same value all across)\n",
        "- Parsed date and created two new columns- month and year\n",
        "\n",
        "Numeric Columns:\n",
        "\n",
        "- List all numerical columns\n",
        "- Impute gps height, lat/long using grouped means\n",
        "- Impute construction year and population using grouped mean\n",
        "- Created new column, age, using year recorded - construction year. Imputed negative vlaues of age\n",
        "- Created new column, 'season' using the month column. \n",
        "- Using DBScan to create clusters for lat/long. Didn't do anything for accuracy\n",
        "\n",
        "Categorical Columns\n",
        "\n",
        "- Converted all strings to lower case \n",
        "- Split into columns that have too many unique values vs not too many unique values\n",
        "- Replaced 0s and 'none's with most frequent values\n",
        "- Cleaned up some values that are mostly similar but have typos or entered as different versions. For example: community vs commu. \n",
        "- Dropped some columns that are mostly similar to others\n",
        "\n",
        "Split Train and Test\n",
        "\n",
        "- Seperated train and test csv files after cleaning\n",
        "- Did not do a train-test split to maximize the training data. Used 3 fold cross validation instead. \n",
        "- Label encoded\n",
        "\n",
        "Pipeline:\n",
        "\n",
        "- MyCategoryCoalescer- Customer transformer (Uncle Steve's) to retain top 25 per column and replace the rest as \"Other'\n",
        "- Ordinal Encoder for all category columns\n",
        "- Scaler for numeric columns. Scaler didnt really boost accuracy, IMO. \n",
        "\n",
        "Models: \n",
        "\n",
        "- Trained random forest, xgboost, adaboost, bagging (with base as decision trees), extra trees, LIghGBM, CatBoost\n",
        "- All models have mostly similar accuracies except adaboost. adaboost lower by a few points\n",
        "- Stacking all five models gave the best accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP3UXc_RUqBO"
      },
      "source": [
        "# **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1XbJ_qBA-LM"
      },
      "outputs": [],
      "source": [
        "# Merged train and test for preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo3hwERoBE0-"
      },
      "outputs": [],
      "source": [
        "values = pd.read_csv(\"https://raw.githubusercontent.com/hsaripalli/Pump-It-Up/main/Training_set_values.csv\")\n",
        "labels = pd.read_csv(\"https://raw.githubusercontent.com/hsaripalli/Pump-It-Up/main/Training_set_labels.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/hsaripalli/Pump-It-Up/main/Test_set_values.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbrAfgWOUpcG"
      },
      "outputs": [],
      "source": [
        "# Merge train and test\n",
        "\n",
        "values['train'] = True\n",
        "test['test'] = True\n",
        "data = pd.concat([values, test], ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd6HBXOnUpZr"
      },
      "outputs": [],
      "source": [
        "#Drop columns\n",
        "\n",
        "columns_to_drop = ['num_private', 'recorded_by']\n",
        "data = data.drop(columns_to_drop, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MrMxnEOUpXi"
      },
      "outputs": [],
      "source": [
        "#Parse dates\n",
        "\n",
        "data['date_recorded' + '_year'] = pd.to_datetime(data['date_recorded']).dt.year \n",
        "data['date_recorded' + '_month'] = pd.to_datetime(data['date_recorded']).dt.month\n",
        "data = data.drop('date_recorded', axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UWQ1i_hUzOl"
      },
      "source": [
        "# **Data Cleaning - Numerical Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIubhySHf2Pn"
      },
      "outputs": [],
      "source": [
        "numeric_columns = data.select_dtypes(exclude = 'object').columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2lUKu5N-78q"
      },
      "outputs": [],
      "source": [
        "# Impute small latitude values with 0\n",
        "data.loc[data['latitude'] > -0.5, 'latitude'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edqt76E5UpVX"
      },
      "outputs": [],
      "source": [
        "# gps height and longitude: impute 0 and nan with grouped mean\n",
        "col1 = ['gps_height', 'longitude', 'latitude']\n",
        "data[col1] = data[col1].replace(0, np.nan)\n",
        "for i in col1:\n",
        "    data[i] = data[i].fillna(data.groupby('subvillage')[i].transform('mean'))\n",
        "    data[i] = data[i].fillna(data.groupby('ward')[i].transform('mean'))\n",
        "    data[i] = data[i].fillna(data.groupby('lga')[i].transform('mean'))\n",
        "    data[i] = data[i].fillna(data.groupby('region')[i].transform('mean'))\n",
        "    data[i] = data[i].fillna(data.groupby('basin')[i].transform('mean'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv5XoCYOfN-6"
      },
      "outputs": [],
      "source": [
        "# construction year and population: impute 0 and nan with most frequent\n",
        "col2 = ['construction_year', 'population']\n",
        "data[col2] = data[col2].replace(0, np.nan)\n",
        "for i in col2:\n",
        "    data[i] = round(data[i].fillna(data.groupby('subvillage')[i].transform('mean')))\n",
        "    data[i] = round(data[i].fillna(data.groupby('ward')[i].transform('mean')))\n",
        "    data[i] = round(data[i].fillna(data.groupby('lga')[i].transform('mean')))\n",
        "    data[i] = round(data[i].fillna(data.groupby('region')[i].transform('mean')))\n",
        "    data[i] = round(data[i].fillna(data.groupby('basin')[i].transform('mean')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7QE-NTBhOTo"
      },
      "outputs": [],
      "source": [
        "# Add age = date recordced - construction year\n",
        "# Impute negative age with 1\n",
        "data['age'] = data['date_recorded_year'] - data['construction_year']\n",
        "data.loc[data['age'] < 0, 'age'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B007lT9Tk2PJ",
        "outputId": "aa554722-7b9d-40df-b06e-91a33d088e63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        2.0\n",
              "1        2.0\n",
              "2        1.0\n",
              "3        1.0\n",
              "4        3.0\n",
              "        ... \n",
              "74245    1.0\n",
              "74246    2.0\n",
              "74247    2.0\n",
              "74248    1.0\n",
              "74249    1.0\n",
              "Name: season, Length: 74250, dtype: float64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Jan and Feb short dry season\n",
        "# long rains lasts during about March, April and May \n",
        "# long dry season lasts throughout June, July, August, September and October \n",
        "# During November and December there's another rainy season: the 'short rains'\n",
        "\n",
        "data.loc[(data['date_recorded_month'] >= 1) & (data['date_recorded_month'] <= 2), 'season'] = 1\n",
        "data.loc[(data['date_recorded_month'] >= 3) & (data['date_recorded_month'] <= 5), 'season'] = 2\n",
        "data.loc[(data['date_recorded_month'] >= 6) & (data['date_recorded_month'] <= 10), 'season'] = 3\n",
        "data.loc[(data['date_recorded_month'] >= 11) & (data['date_recorded_month'] <= 12), 'season'] = 4\n",
        "\n",
        "data['season']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANV3yc9bQpiP"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "clusters = 15\n",
        "kmeans = KMeans(n_clusters=clusters, random_state=0).fit(data[['latitude', 'longitude']].values)\n",
        "kmean_feats = pd.DataFrame(kmeans.fit_transform(data[['latitude', 'longitude']].values), columns=['gspatial_' + str(i) for i in range(clusters)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v05adyNrRvg_"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([data, kmean_feats], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ4aNgHaV7y-"
      },
      "source": [
        "# **Data Cleaning - Categorical Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuKEK_rqUpSt"
      },
      "outputs": [],
      "source": [
        "categorical_columns = data.select_dtypes(include = 'object').columns.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkdfyU0cWNkW"
      },
      "source": [
        "*Dealing with columns that contain too many unique values*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvx3zQadXYnd"
      },
      "outputs": [],
      "source": [
        "# TOO MANY UNIQUE VALUES\n",
        "#funder                    2140\n",
        "#installer                 2410\n",
        "#wpt_name                 45684\n",
        "#subvillage               21425\n",
        "#lga                        125\n",
        "#ward                      2098\n",
        "#scheme name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6uqV6XXUpPy"
      },
      "outputs": [],
      "source": [
        "# convert to lowercase\n",
        "col3 = ['funder', 'installer','wpt_name', 'basin', 'subvillage', 'region',\n",
        "                 'lga', 'ward','scheme_management', 'extraction_type','extraction_type_group',\n",
        "                 'extraction_type_class','management','management_group','payment','payment_type',\n",
        "                 'water_quality', 'quality_group','quantity','quantity_group','source','source_type', \n",
        "                 'source_class','waterpoint_type','waterpoint_type_group', 'scheme_name']\n",
        "for i in col3:\n",
        "  data[i] = data[i].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfBAWReBdlki"
      },
      "outputs": [],
      "source": [
        "# fill na with most frequest\n",
        "col4 = ['funder', 'installer', 'wpt_name', 'subvillage', 'lga', 'ward', 'scheme_name']\n",
        "data[col4] = data[col4].replace(to_replace = ('0', 'none'), value = np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0hJ6z1Gdg-j"
      },
      "outputs": [],
      "source": [
        "data['installer'] = data['installer'].replace(to_replace = ('gover'), value = 'government')\n",
        "data['installer'] = data['installer'].replace(to_replace = ('commu'), value = 'community')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQFrvu59deDG"
      },
      "outputs": [],
      "source": [
        "for i in col4:\n",
        "    data[i] = data[i].fillna(data[i].mode()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StcpnG6hWtx_"
      },
      "source": [
        "Dealing with columns containing **not** too many unique values\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4z5Apc9UpJB"
      },
      "outputs": [],
      "source": [
        "# Not too many unique values\n",
        "#basin                        9\n",
        "#region                      21\n",
        "#region_code                 27\n",
        "#district_code               20\n",
        "#public_meeting               2\n",
        "#scheme_management           12\n",
        "#permit                       2\n",
        "#construction_year           55\n",
        "#extraction_type             18\n",
        "#extraction_type_group       13\n",
        "#extraction_type_class        7\n",
        "#management                  12\n",
        "#management_group             5\n",
        "#payment                      7\n",
        "#payment_type                 7\n",
        "#water_quality                8\n",
        "#quality_group                6\n",
        "#quantity                     5\n",
        "#quantity_group               5\n",
        "#source                      10\n",
        "#source_type                  7\n",
        "#source_class                 3\n",
        "#waterpoint_type              7\n",
        "#waterpoint_type_group        6\n",
        "#train                        1\n",
        "#test                         1\n",
        "#date_recorded_year           6\n",
        "#date_recorded_month         12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnw4UJqKXyh-"
      },
      "outputs": [],
      "source": [
        "#public_meeting               2\n",
        "#scheme_management           12\n",
        "#permit                       2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU2VtLq-vM5C"
      },
      "outputs": [],
      "source": [
        "col5 = ['public_meeting', 'permit']\n",
        "for i in col5:\n",
        "    data[i] = data[i].fillna(data[i].mode()[0])\n",
        "    data[i] = data[i].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEOEHSzyUpAB"
      },
      "outputs": [],
      "source": [
        "# public meeting and scheme management: fill na with most frequest\n",
        "\n",
        "data['scheme_management'] = data['scheme_management'].replace(to_replace = (np.nan, 'none'), value = 'other')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v92K5BXoUoap"
      },
      "outputs": [],
      "source": [
        "#extraction_type             18\n",
        "#extraction_type_group       13\n",
        "#extraction_type_class        7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaZeQzeCUoKb"
      },
      "outputs": [],
      "source": [
        "# clean/ replace some values in extraction_type column\n",
        "\n",
        "data = data.replace({'extraction_type': \n",
        "                     {'cemo': 'other motorpump',\n",
        "                      'climax': 'other motorpump',\n",
        "                      'india mark ii': 'india mark',\n",
        "                      'india mark iii': 'india mark',\n",
        "                      'other - mkulima/shinyanga': 'other handpump',\n",
        "                      'other - play pump': 'other handpump',\n",
        "                      'other - rope pump': 'rope pump',\n",
        "                      'other - swn 81': 'swn',\n",
        "                      'swn 80': 'swn'\n",
        "                      }})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt0aqLJHYR3a"
      },
      "outputs": [],
      "source": [
        "# describe columns (run one at a time)\n",
        "\n",
        "#data[['extraction_type', 'extraction_type_group', 'extraction_type_class']].groupby('extraction_type_group').describe()\n",
        "#data[['payment', 'payment_type']].groupby('payment').describe()\n",
        "#data[['water_quality', 'quality_group']].groupby('water_quality').describe()\n",
        "#data[['quantity', 'quantity_group']].groupby('quantity').describe()\n",
        "#data[['source', 'source_type', 'source_class']].groupby('source').describe()\n",
        "#data[['waterpoint_type', 'waterpoint_type_group']].groupby('waterpoint_type').describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPsL2aTlYkJX"
      },
      "outputs": [],
      "source": [
        "col6 = ['extraction_type_group', 'payment_type', 'quality_group', 'quantity_group', 'source_type', 'waterpoint_type_group']\n",
        "data = data.drop(col6, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbJ6oDzMYy1u",
        "outputId": "ceabde90-bc5a-4998-81ba-d615eb079dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 74250 entries, 0 to 74249\n",
            "Data columns (total 52 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   id                     74250 non-null  int64  \n",
            " 1   amount_tsh             74250 non-null  float64\n",
            " 2   funder                 74250 non-null  object \n",
            " 3   gps_height             74250 non-null  float64\n",
            " 4   installer              74250 non-null  object \n",
            " 5   longitude              74250 non-null  float64\n",
            " 6   latitude               74250 non-null  float64\n",
            " 7   wpt_name               74250 non-null  object \n",
            " 8   basin                  74250 non-null  object \n",
            " 9   subvillage             74250 non-null  object \n",
            " 10  region                 74250 non-null  object \n",
            " 11  region_code            74250 non-null  int64  \n",
            " 12  district_code          74250 non-null  int64  \n",
            " 13  lga                    74250 non-null  object \n",
            " 14  ward                   74250 non-null  object \n",
            " 15  population             74250 non-null  float64\n",
            " 16  public_meeting         74250 non-null  object \n",
            " 17  scheme_management      74250 non-null  object \n",
            " 18  scheme_name            74250 non-null  object \n",
            " 19  permit                 74250 non-null  object \n",
            " 20  construction_year      74250 non-null  float64\n",
            " 21  extraction_type        74250 non-null  object \n",
            " 22  extraction_type_class  74250 non-null  object \n",
            " 23  management             74250 non-null  object \n",
            " 24  management_group       74250 non-null  object \n",
            " 25  payment                74250 non-null  object \n",
            " 26  water_quality          74250 non-null  object \n",
            " 27  quantity               74250 non-null  object \n",
            " 28  source                 74250 non-null  object \n",
            " 29  source_class           74250 non-null  object \n",
            " 30  waterpoint_type        74250 non-null  object \n",
            " 31  train                  59400 non-null  object \n",
            " 32  test                   14850 non-null  object \n",
            " 33  date_recorded_year     74250 non-null  int64  \n",
            " 34  date_recorded_month    74250 non-null  int64  \n",
            " 35  age                    74250 non-null  float64\n",
            " 36  season                 74250 non-null  float64\n",
            " 37  gspatial_0             74250 non-null  float64\n",
            " 38  gspatial_1             74250 non-null  float64\n",
            " 39  gspatial_2             74250 non-null  float64\n",
            " 40  gspatial_3             74250 non-null  float64\n",
            " 41  gspatial_4             74250 non-null  float64\n",
            " 42  gspatial_5             74250 non-null  float64\n",
            " 43  gspatial_6             74250 non-null  float64\n",
            " 44  gspatial_7             74250 non-null  float64\n",
            " 45  gspatial_8             74250 non-null  float64\n",
            " 46  gspatial_9             74250 non-null  float64\n",
            " 47  gspatial_10            74250 non-null  float64\n",
            " 48  gspatial_11            74250 non-null  float64\n",
            " 49  gspatial_12            74250 non-null  float64\n",
            " 50  gspatial_13            74250 non-null  float64\n",
            " 51  gspatial_14            74250 non-null  float64\n",
            "dtypes: float64(23), int64(5), object(24)\n",
            "memory usage: 29.5+ MB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgdU3eYfY_qx"
      },
      "source": [
        "# **Split Train and Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga0SHXe_czor"
      },
      "outputs": [],
      "source": [
        "# Reverse split merged and clean data into train and test\n",
        "train_values = data[data['train'] == True]\n",
        "test = data[data['test'] == True]\n",
        "train_values = train_values.drop(['train', 'test'], axis = 1)\n",
        "test = test.drop(['train', 'test'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWI3PqFLczeU"
      },
      "outputs": [],
      "source": [
        "test_set = test.drop('id', axis = 1)\n",
        "x = train_values.drop('id', axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fesor1KzFq1a"
      },
      "outputs": [],
      "source": [
        "X = x.copy()\n",
        "y = pd.DataFrame(labels['status_group'])\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y.values.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaPezdMMNUPg",
        "outputId": "b0bf16bc-3d27-4e87-e407-27a24975b973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'functional': 0, 'functional needs repair': 1, 'non functional': 2}\n"
          ]
        }
      ],
      "source": [
        "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(le_name_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ign0M1rdBaCj"
      },
      "outputs": [],
      "source": [
        "# Split train and test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.01, random_state = 123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDtIBfrRyFM_",
        "outputId": "97ad35dd-1106-4677-e8dd-c05e028cad84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 14850 entries, 36801 to 56271\n",
            "Data columns (total 49 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   amount_tsh             14850 non-null  float64\n",
            " 1   funder                 14850 non-null  object \n",
            " 2   gps_height             14850 non-null  float64\n",
            " 3   installer              14850 non-null  object \n",
            " 4   longitude              14850 non-null  float64\n",
            " 5   latitude               14850 non-null  float64\n",
            " 6   wpt_name               14850 non-null  object \n",
            " 7   basin                  14850 non-null  object \n",
            " 8   subvillage             14850 non-null  object \n",
            " 9   region                 14850 non-null  object \n",
            " 10  region_code            14850 non-null  int64  \n",
            " 11  district_code          14850 non-null  int64  \n",
            " 12  lga                    14850 non-null  object \n",
            " 13  ward                   14850 non-null  object \n",
            " 14  population             14850 non-null  float64\n",
            " 15  public_meeting         14850 non-null  object \n",
            " 16  scheme_management      14850 non-null  object \n",
            " 17  scheme_name            14850 non-null  object \n",
            " 18  permit                 14850 non-null  object \n",
            " 19  construction_year      14850 non-null  float64\n",
            " 20  extraction_type        14850 non-null  object \n",
            " 21  extraction_type_class  14850 non-null  object \n",
            " 22  management             14850 non-null  object \n",
            " 23  management_group       14850 non-null  object \n",
            " 24  payment                14850 non-null  object \n",
            " 25  water_quality          14850 non-null  object \n",
            " 26  quantity               14850 non-null  object \n",
            " 27  source                 14850 non-null  object \n",
            " 28  source_class           14850 non-null  object \n",
            " 29  waterpoint_type        14850 non-null  object \n",
            " 30  date_recorded_year     14850 non-null  int64  \n",
            " 31  date_recorded_month    14850 non-null  int64  \n",
            " 32  age                    14850 non-null  float64\n",
            " 33  season                 14850 non-null  float64\n",
            " 34  gspatial_0             14850 non-null  float64\n",
            " 35  gspatial_1             14850 non-null  float64\n",
            " 36  gspatial_2             14850 non-null  float64\n",
            " 37  gspatial_3             14850 non-null  float64\n",
            " 38  gspatial_4             14850 non-null  float64\n",
            " 39  gspatial_5             14850 non-null  float64\n",
            " 40  gspatial_6             14850 non-null  float64\n",
            " 41  gspatial_7             14850 non-null  float64\n",
            " 42  gspatial_8             14850 non-null  float64\n",
            " 43  gspatial_9             14850 non-null  float64\n",
            " 44  gspatial_10            14850 non-null  float64\n",
            " 45  gspatial_11            14850 non-null  float64\n",
            " 46  gspatial_12            14850 non-null  float64\n",
            " 47  gspatial_13            14850 non-null  float64\n",
            " 48  gspatial_14            14850 non-null  float64\n",
            "dtypes: float64(23), int64(4), object(22)\n",
            "memory usage: 5.7+ MB\n"
          ]
        }
      ],
      "source": [
        "X_test.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLYiGau30x6W"
      },
      "source": [
        "# **Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGmnDxWjBFLp"
      },
      "outputs": [],
      "source": [
        "#Uncle Steve's Custom Transformer for Category Coalescing\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class MyCategoryCoalescer(BaseEstimator, TransformerMixin):\n",
        "    # Coalesces (smushes/condenses) rare levels of a categorical \n",
        "    # feature into \"__OTHER__\".\n",
        "    #\n",
        "    # Will leave the `keep_top` most frequent levels unchanged; the rest\n",
        "    # will be changed to `\"__OTHER__\"`.\n",
        "    #\n",
        "    # Note that there was a design choice: either have the user\n",
        "    # pass in the names of the columns to operate one (which I've done here), \n",
        "    # or just operate on all the columns (and have the user be responsible for\n",
        "    # passing in a subset of the dataframe). Pros and cons to each and there's\n",
        "    # note a singe best answer.\n",
        "    \n",
        "    def __init__(self, cat_cols=[], keep_top=25):\n",
        "        self.cat_cols = cat_cols\n",
        "        self.keep_top = keep_top\n",
        "        \n",
        "        # For each cat_col, this dict will hold an list of the most-frequent \n",
        "        # levels\n",
        "        self.top_n_values = {}\n",
        "            \n",
        "    def get_top_n_values(self, X, col, n=25):\n",
        "        # A helper function to do the actual work.\n",
        "\n",
        "        # Get the sorted value counts for the column\n",
        "        vc = X[col].value_counts(sort=True, ascending=False)\n",
        "\n",
        "        # Get the actual values\n",
        "        vals = list(vc.index)\n",
        "        if len(vals) > n:\n",
        "            top_values = vals[0:n]\n",
        "        else:\n",
        "            top_values =  vals\n",
        "\n",
        "        # Debug printing.\n",
        "        #print(\"Top n={} values for column={}:\".format(n, col))\n",
        "        #print(top_values)\n",
        "        return top_values\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "\n",
        "        # Find the top n values for each cateogircal column\n",
        "        for col in self.cat_cols:\n",
        "            self.top_n_values[col] = self.get_top_n_values(X, col, n=self.keep_top)\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        _X = X.copy()\n",
        "        _X[self.cat_cols] = _X[self.cat_cols].astype('category')\n",
        "        for c in self.cat_cols:\n",
        "            _X[c] = _X[c].cat.add_categories('__OTHER__')\n",
        "            _X.loc[~_X[c].isin(self.top_n_values[c]), c] = \"__OTHER__\"\n",
        "        return _X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvyIKdaWBFOU"
      },
      "outputs": [],
      "source": [
        "# Model fit\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import make_column_selector\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "categorical_cols = ['funder', 'installer', 'wpt_name', 'basin', 'subvillage', 'region', 'lga', 'ward', 'scheme_management',\n",
        "                    'extraction_type','extraction_type_class', 'management', 'management_group', 'payment', 'water_quality',\n",
        "                    'quantity','source','source_class','waterpoint_type', 'permit', 'public_meeting', 'scheme_name']\n",
        "\n",
        "columns_to_coal = ['funder','installer', 'subvillage', 'lga', 'ward', 'wpt_name', 'scheme_name']\n",
        "\n",
        "columns_to_scale = ['population', 'gps_height', 'latitude', 'longitude']\n",
        "\n",
        "coalescer = MyCategoryCoalescer(cat_cols=columns_to_coal, keep_top=25)\n",
        "encoder = OrdinalEncoder()\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "cat_transformer = Pipeline([\n",
        "                            ('coalescer', coalescer),                      \n",
        "                            ('encoder', encoder)\n",
        "                           ])\n",
        "\n",
        "preprocessor = Pipeline(steps = [\n",
        "                                 ('ct', ColumnTransformer(\n",
        "                                     transformers=[\n",
        "                                                   ('categorical', cat_transformer, categorical_cols),\n",
        "                                                   ('scale', scaler, columns_to_scale)\n",
        "                                                   ], \n",
        "                                                   remainder = 'passthrough', \n",
        "                                                   sparse_threshold =0)),\n",
        "                                 ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmCFcixJRDaf"
      },
      "source": [
        "# *Random Forest (optuna tuned)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWsWvka-lobb"
      },
      "outputs": [],
      "source": [
        "# Random Forest Tuned\n",
        "# Random Forest Tuned Hyper Parameters\n",
        "# {'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__n_estimators': 1000}\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(criterion = 'gini',\n",
        "                            n_estimators = 536,\n",
        "                            min_samples_split = 8,\n",
        "                            max_depth = 20,\n",
        "                            random_state = 42)\n",
        "\n",
        "rf_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('rf', rf)])\n",
        "rf_pipeline.fit(X_train,y_train)\n",
        "y_pred_rf_pipeline = rf_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYrILFEZlxnN",
        "outputId": "4ace3a53-ce07-4b90-ff8b-7fdc6f5b9bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of RF = 0.8418\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy of RF = {:.4f}\".format(accuracy_score(y_test, y_pred_rf_pipeline)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMxT8BFBI3dh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(y_test, y_pred_rf_pipeline))\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred_rf_pipeline))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "M28f3uJPEHyD",
        "outputId": "033bf308-1f85-412b-ff5e-7ad3148f7df0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-11-16 10:40:02,033]\u001b[0m A new study created in memory with name: no-name-4e962812-b08c-43ac-916e-97406778cbe3\u001b[0m\n",
            "\u001b[32m[I 2021-11-16 10:42:50,490]\u001b[0m Trial 0 finished with value: 0.7937205387205388 and parameters: {'criterion': 'gini', 'min_samples_split': 10, 'n_estimators': 470, 'max_depth': 14}. Best is trial 0 with value: 0.7937205387205388.\u001b[0m\n",
            "\u001b[32m[I 2021-11-16 11:01:18,854]\u001b[0m Trial 1 finished with value: 0.806969696969697 and parameters: {'criterion': 'entropy', 'min_samples_split': 4, 'n_estimators': 1430, 'max_depth': 49}. Best is trial 1 with value: 0.806969696969697.\u001b[0m\n",
            "\u001b[32m[I 2021-11-16 11:07:59,682]\u001b[0m Trial 2 finished with value: 0.8093771043771044 and parameters: {'criterion': 'entropy', 'min_samples_split': 7, 'n_estimators': 516, 'max_depth': 26}. Best is trial 2 with value: 0.8093771043771044.\u001b[0m\n",
            "\u001b[32m[I 2021-11-16 11:13:46,526]\u001b[0m Trial 3 finished with value: 0.8093602693602694 and parameters: {'criterion': 'gini', 'min_samples_split': 9, 'n_estimators': 580, 'max_depth': 46}. Best is trial 2 with value: 0.8093771043771044.\u001b[0m\n",
            "\u001b[32m[I 2021-11-16 11:22:16,394]\u001b[0m Trial 4 finished with value: 0.8063636363636363 and parameters: {'criterion': 'entropy', 'min_samples_split': 3, 'n_estimators': 572, 'max_depth': 26}. Best is trial 2 with value: 0.8093771043771044.\u001b[0m\n",
            "\u001b[32m[I 2021-11-16 11:26:30,946]\u001b[0m Trial 5 finished with value: 0.7384006734006734 and parameters: {'criterion': 'gini', 'min_samples_split': 9, 'n_estimators': 1031, 'max_depth': 7}. Best is trial 2 with value: 0.8093771043771044.\u001b[0m\n",
            "\u001b[32m[I 2021-11-16 11:38:32,049]\u001b[0m Trial 6 finished with value: 0.795117845117845 and parameters: {'criterion': 'gini', 'min_samples_split': 8, 'n_estimators': 1497, 'max_depth': 14}. Best is trial 2 with value: 0.8093771043771044.\u001b[0m\n",
            "\u001b[32m[I 2021-11-16 11:39:44,951]\u001b[0m Trial 7 finished with value: 0.7262121212121212 and parameters: {'criterion': 'gini', 'min_samples_split': 6, 'n_estimators': 367, 'max_depth': 5}. Best is trial 2 with value: 0.8093771043771044.\u001b[0m\n",
            "\u001b[32m[I 2021-11-16 11:52:02,782]\u001b[0m Trial 8 finished with value: 0.8074074074074075 and parameters: {'criterion': 'gini', 'min_samples_split': 5, 'n_estimators': 1325, 'max_depth': 47}. Best is trial 2 with value: 0.8093771043771044.\u001b[0m\n",
            "\u001b[32m[I 2021-11-16 12:04:46,559]\u001b[0m Trial 9 finished with value: 0.8087373737373736 and parameters: {'criterion': 'gini', 'min_samples_split': 10, 'n_estimators': 1407, 'max_depth': 49}. Best is trial 2 with value: 0.8093771043771044.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of finished trials: 10\n",
            "Best trial:\n",
            "  Value: 0.8093771043771044\n",
            "  Params: \n",
            "    criterion: entropy\n",
            "    min_samples_split: 7\n",
            "    n_estimators: 516\n",
            "    max_depth: 26\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "def objective(trial):\n",
        "    \n",
        "\n",
        "    param = {\n",
        "        \"criterion\": trial.suggest_categorical(\"criterion\", ['gini', 'entropy']),\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2,10),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200,1500),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 50)\n",
        "    }\n",
        "\n",
        "    rf = RandomForestClassifier(**param)\n",
        "    rf_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('rf', rf)])\n",
        "    \n",
        "    return cross_val_score(rf_pipeline, X, y, cv = 3).mean()\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2gjF7_lyEHyD"
      },
      "outputs": [],
      "source": [
        "#Number of finished trials: 10\n",
        "#Best trial:\n",
        "#  Value: 0.8093771043771044\n",
        "#  Params: \n",
        "#    criterion: entropy\n",
        "#    min_samples_split: 7\n",
        "#    n_estimators: 516\n",
        "#    max_depth: 26\n",
        "        \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(criterion = 'gini',\n",
        "                            n_estimators = 516,\n",
        "                            min_samples_split = 7,\n",
        "                            max_depth = 26,\n",
        "                            random_state = 42)\n",
        "\n",
        "rf_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('rf', rf)])\n",
        "rf_pipeline.fit(X_train,y_train)\n",
        "y_pred_rf_pipeline = rf_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICxuagXoEHyE",
        "outputId": "fea4bbf8-01b7-4415-c7b3-6616cba21c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of RF = 0.8468\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy of RF = {:.4f}\".format(accuracy_score(y_test, y_pred_rf_pipeline)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TltV_GYG2aWI"
      },
      "source": [
        "# *LGBM (optuna tuned)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51BXUNMJUj3n",
        "outputId": "60d3f4ce-500d-47f0-fbd6-001d3d851f82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HariSaripalli\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] feature_fraction is set=0.5633644014015632, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5633644014015632\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=17, min_child_samples=47 will be ignored. Current value: min_data_in_leaf=17\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.2899315163770417e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.2899315163770417e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6953776886469089, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6953776886469089\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.6273452242794607e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.6273452242794607e-06\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n"
          ]
        }
      ],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgbm = LGBMClassifier(boosting_type = 'gbdt',\n",
        "                      objective = 'multiclass',\n",
        "                      num_class = 3,\n",
        "                      metric = 'multi_error',\n",
        "                      num_iterations = 200,\n",
        "                      lambda_l1 =  2.2899315163770417e-06,\n",
        "                      lambda_l2 =  2.6273452242794607e-06,\n",
        "                      num_leaves = 239,\n",
        "                      feature_fraction = 0.5633644014015632,\n",
        "                      learning_rate = 0.06012805964180289,\n",
        "                      bagging_fraction = 0.6953776886469089,\n",
        "                      bagging_freq = 6,\n",
        "                      min_child_samples = 47,\n",
        "                      min_data_in_leaf = 17,\n",
        "                      max_depth = 46\n",
        "                      )\n",
        "\n",
        "lgbm_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('lgbm', lgbm)])\n",
        "lgbm_pipeline.fit(X_train,y_train)\n",
        "y_pred_lgbm_pipeline = lgbm_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed_Gzsl7EHyE",
        "outputId": "8b4b8664-ccb4-4a61-f9ac-01173204f6bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of LGBM   = 0.8333\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy of LGBM   = {:.4f}\".format(accuracy_score(y_test, y_pred_lgbm_pipeline)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCyMEISR6WoC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(y_test, y_pred_lgbm_pipeline))\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred_lgbm_pipeline))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TrlLOYo5Jqn"
      },
      "source": [
        "# *Catboost (hyperparameter tuned)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7m8v4ANiHLo"
      },
      "outputs": [],
      "source": [
        "pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ4n0d7_5vop"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "cat = CatBoostClassifier(depth = 10,\n",
        "                        iterations = 500,\n",
        "                         learning_rate = 0.05,\n",
        "                        random_state = 42)\n",
        "\n",
        "cat_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('catboost', cat)])\n",
        "cat_pipeline.fit(X_train,y_train)\n",
        "y_pred_cat_pipeline = cat_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zjzahvez0gf9"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of Catboost   = {:.4f}\".format(accuracy_score(y_test, y_pred_cat_pipeline)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sfGYNv-EHyF"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "def objective(trial):\n",
        "    \n",
        "\n",
        "    param = {\n",
        "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
        "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
        "        \"bootstrap_type\": trial.suggest_categorical(\n",
        "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
        "        ),\n",
        "        \n",
        "        \"used_ram_limit\": \"2gb\",\n",
        "    }\n",
        "\n",
        "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
        "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
        "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
        "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
        "\n",
        "    cat = CatBoostClassifier(**param)\n",
        "    cat_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('catboost', cat)])\n",
        "    \n",
        "    return cross_val_score(cat_pipeline, X, y, cv = 3).mean()\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "xNB34IlLEHyG"
      },
      "outputs": [],
      "source": [
        "#Number of finished trials: 10\n",
        "#Best trial:\n",
        "#  Value: 0.8049831649831649\n",
        "#  Params: \n",
        "#    colsample_bylevel: 0.07369920952387737\n",
        "#    depth: 12\n",
        "#    boosting_type: Plain\n",
        "#   bootstrap_type: MVS\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "cat = CatBoostClassifier(colsample_bylevel = 0.073699209523,\n",
        "                         depth = 12,\n",
        "                         boosting_type = 'Plain',\n",
        "                         bootstrap_type = 'MVS',\n",
        "                        random_state = 42)\n",
        "\n",
        "cat_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('catboost', cat)])\n",
        "cat_pipeline.fit(X_train,y_train)\n",
        "y_pred_cat_pipeline = cat_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFUanmMaEHyG",
        "outputId": "fd98713c-1c49-4511-fbbb-be54bdbb7e75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Catboost   = 0.8316\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy of Catboost   = {:.4f}\".format(accuracy_score(y_test, y_pred_cat_pipeline)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM-My2-Q95hR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(y_test, y_pred_cat_pipeline))\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred_cat_pipeline))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PswY1M-1Q_cB"
      },
      "source": [
        "# *XG Boost*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2nWiNKm4Gw0"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "#Number of finished trials: 30\n",
        "#Best trial:\n",
        "#  Value: 0.8013131313131314\n",
        "#  Params: \n",
        "#    booster: dart\n",
        "#    lambda: 4.572637572518502e-07\n",
        "#    alpha: 6.037662427475617e-05\n",
        "#    subsample: 0.7162353406216146\n",
        "#    colsample_bytree: 0.8486248682584188\n",
        "#    max_depth: 7\n",
        "#    min_child_weight: 9\n",
        "#    eta: 0.3563123559925298\n",
        "#    gamma: 5.017895421049517e-05\n",
        "#    grow_policy: depthwise\n",
        "#    sample_type: uniform\n",
        "#    normalize_type: forest\n",
        "#    rate_drop: 0.012104590680294654\n",
        "#    skip_drop: 0.00036189755567904127\n",
        "\n",
        "xg = XGBClassifier(booster = 'dart',\n",
        "                   alpha =  6.037662427475617e-05,\n",
        "                   subsample = 0.7162353406216146,\n",
        "                   colsample_bytree = 0.8486248682584188,\n",
        "                   max_depth = 7,\n",
        "                   min_child_weight = 9,\n",
        "                   eta = 0.3563123559925298,\n",
        "                   gamma = 5.017895421049517e-05,\n",
        "                   grow_policy = 'depthwise',\n",
        "                   sample_type = 'uniform',\n",
        "                   normalize_type = 'forest',\n",
        "                   rate_drop = 0.012104590680294654,\n",
        "                   skip_drop = 0.00036189755567904127,\n",
        "                   objective='multi:softmax',\n",
        "                   use_label_encoder = False)\n",
        "\n",
        "xg_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('xgboost', xg)])\n",
        "xg_pipeline.fit(X_train,y_train)\n",
        "y_pred_xg_pipeline = xg_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dkkoDbF7vU4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of XGB   = {:.4f}\".format(accuracy_score(y_test, y_pred_xg_pipeline)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ckmr4NCQEHyH"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    \n",
        "  \n",
        "    param = {\n",
        "        \"verbosity\": 0,\n",
        "        \"objective\": \"binary:logistic\",\n",
        "        # use exact for small dataset.\n",
        "        \"tree_method\": \"exact\",\n",
        "        # defines booster, gblinear for linear functions.\n",
        "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
        "        # L2 regularization weight.\n",
        "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
        "        # L1 regularization weight.\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
        "        # sampling ratio for training data.\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
        "        # sampling according to each tree.\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
        "    }\n",
        "\n",
        "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
        "        # maximum depth of the tree, signifies complexity of the tree.\n",
        "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
        "        # minimum child weight, larger the term more conservative the tree.\n",
        "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
        "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
        "        # defines how selective algorithm is.\n",
        "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
        "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
        "\n",
        "    if param[\"booster\"] == \"dart\":\n",
        "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
        "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
        "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
        "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
        "\n",
        "    xg = XGBClassifier(**param)\n",
        "    xg_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('xgboost', xg)])\n",
        "    \n",
        "    return cross_val_score(xg_pipeline, X, y, cv = 3).mean()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=30)\n",
        "\n",
        "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4-RLCKv4xzn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(y_test, y_pred_xg_pipeline))\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred_xg_pipeline))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbirwI1wPv01"
      },
      "source": [
        "# *Extra Trees*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XqHtkMpG_Hk"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "xt = ExtraTreesClassifier(n_estimators=200,\n",
        "                          random_state=42)\n",
        "\n",
        "xt_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('extra trees', xt)])\n",
        "xt_pipeline.fit(X_train,y_train)\n",
        "y_pred_xt_pipeline = xt_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnS_HScBHVkh"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of EXTRA TREES   = {:.4f}\".format(accuracy_score(y_test, y_pred_xt_pipeline)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJ4Bmrdg5TbN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(y_test, y_pred_xt_pipeline))\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred_xt_pipeline))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xrv_OlrRIip"
      },
      "source": [
        "# *Bagging*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d4qqAXIRWVa"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "bag =  BaggingClassifier(n_estimators=100,\n",
        "                         max_features = 0.5,\n",
        "                         random_state=42)\n",
        "\n",
        "bag_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('bagging', bag)])\n",
        "bag_pipeline.fit(X_train,y_train)\n",
        "y_pred_bag_pipeline = bag_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11oLN2P8RvnD"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of BAGGING   = {:.4f}\".format(accuracy_score(y_test, y_pred_bag_pipeline)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzZNh_Ft5pLO"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(y_test, y_pred_bag_pipeline))\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred_bag_pipeline))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls2PLicY_VfM"
      },
      "source": [
        "# *Voting Classifier*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zUH-mk7S5Qc"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "est_list = [('rf', rf), ('xgboost', xg), ('lgbm', lgbm)]\n",
        "\n",
        "\n",
        "vclf = VotingClassifier(estimators = est_list, voting='soft')\n",
        "\n",
        "\n",
        "vote_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('voting', vclf)])\n",
        "\n",
        "vote_pipeline.fit(X_train,y_train)\n",
        "y_pred_vote_pipeline = vote_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owOqqY5kTcr8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of VOTING = {:.4f}\".format(accuracy_score(y_test, y_pred_vote_pipeline)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmPJN0poEHyJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(criterion = 'gini',\n",
        "                            n_estimators = 536,\n",
        "                            min_samples_split = 8,\n",
        "                            max_depth = 20,\n",
        "                            random_state = 42)\n",
        "\n",
        "xg = XGBClassifier(booster = 'dart',\n",
        "                   alpha =  6.037662427475617e-05,\n",
        "                   subsample = 0.7162353406216146,\n",
        "                   colsample_bytree = 0.8486248682584188,\n",
        "                   max_depth = 7,\n",
        "                   min_child_weight = 9,\n",
        "                   eta = 0.3563123559925298,\n",
        "                   gamma = 5.017895421049517e-05,\n",
        "                   grow_policy = 'depthwise',\n",
        "                   sample_type = 'uniform',\n",
        "                   normalize_type = 'forest',\n",
        "                   rate_drop = 0.012104590680294654,\n",
        "                   skip_drop = 0.00036189755567904127,\n",
        "                   objective='multi:softmax',\n",
        "                   use_label_encoder = False)\n",
        "\n",
        "xt = ExtraTreesClassifier(n_estimators=200,\n",
        "                          random_state=42)\n",
        "\n",
        "\n",
        "bag =  BaggingClassifier(n_estimators=100,\n",
        "                         max_features = 0.5,\n",
        "                         random_state=42)\n",
        "\n",
        "cat = CatBoostClassifier(colsample_bylevel = 0.073699209523,\n",
        "                         depth = 12,\n",
        "                         boosting_type = 'Plain',\n",
        "                         bootstrap_type = 'MVS',\n",
        "                        random_state = 42)\n",
        "\n",
        "lgbm = LGBMClassifier(boosting_type = 'gbdt',\n",
        "                      objective = 'multiclass',\n",
        "                      num_class = 3,\n",
        "                      metric = 'multi_error',\n",
        "                      num_iterations = 200,\n",
        "                      lambda_l1 =  2.2899315163770417e-06,\n",
        "                      lambda_l2 =  2.6273452242794607e-06,\n",
        "                      num_leaves = 239,\n",
        "                      feature_fraction = 0.5633644014015632,\n",
        "                      learning_rate = 0.06012805964180289,\n",
        "                      bagging_fraction = 0.6953776886469089,\n",
        "                      bagging_freq = 6,\n",
        "                      min_child_samples = 47,\n",
        "                      min_data_in_leaf = 17,\n",
        "                      max_depth = 46\n",
        "                      )\n",
        "\n",
        "rf_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('rf', rf)])\n",
        "xg_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('xgboost', xg)])\n",
        "xt_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('extra trees', xt)])\n",
        "bag_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('bagging', bag)])\n",
        "cat_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('catboost', cat)])\n",
        "lgbm_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('lgbm', lgbm)])\n",
        "\n",
        "\n",
        "est_list = [('rf', rf), ('xgboost', xg), ('extra trees', xt), ('bagging', bag), ('catboost', cat), ('lgbm', lgbm)]\n",
        "vclf = VotingClassifier(estimators = est_list, voting='soft')\n",
        "\n",
        "\n",
        "vote_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('voting', vclf)])\n",
        "\n",
        "vote_pipeline.fit(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu7MfxMYEHyK"
      },
      "outputs": [],
      "source": [
        "accuracy = cross_val_score(vote_pipeline, X, y, cv = 5).mean()\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cDUZ6XMEHyK",
        "outputId": "0e6b3cbd-9bd1-4645-9ee0-af3deef2cfee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8176430976430977"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxojK-XpEHyK"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of VOTING = {:.4f}\".format(accuracy_score(y_test, y_pred_voting_pipeline)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmRKPB_p5dYk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(y_test, y_pred_vote_pipeline))\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred_vote_pipeline))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxtcJ-ByDLRc"
      },
      "source": [
        "# *Stacking*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtY-BGupDKDs",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#est_list = [('rf', rf), ('xgboost', xg), ('extra trees', xt), ('bagging', bag), ('catboost', cat), ('lgbm', lgbm)]\n",
        "est_list = [('rf', rf), ('xgboost', xg), ('lgbm', lgbm)]\n",
        "\n",
        "sclf = StackingClassifier(estimators = est_list,\n",
        "                          final_estimator = LogisticRegression())\n",
        "\n",
        "stacking_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('stacking', sclf)])\n",
        "\n",
        "stacking_pipeline.fit(X_train,y_train)\n",
        "y_pred_stacking_pipeline = stacking_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRoq_pKYF9S-"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of STACKING = {:.4f}\".format(accuracy_score(y_test, y_pred_stacking_pipeline)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMxZGDADEHyL"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "est_list = [('rf', rf), ('xgboost', xg), ('extra trees', xt), ('bagging', bag), ('catboost', cat), ('lgbm', lgbm)]\n",
        "#est_list = [('rf', rf), ('xgboost', xg), ('lgbm', lgbm)]\n",
        "\n",
        "sclf = StackingClassifier(estimators = est_list,\n",
        "                          final_estimator = LogisticRegression())\n",
        "\n",
        "stacking_pipeline = Pipeline(steps = [('preprocess', preprocessor), ('stacking', sclf)])\n",
        "\n",
        "stacking_pipeline.fit(X_train,y_train)\n",
        "y_pred_stacking_pipeline = stacking_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mobgXpWCEHyL"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of STACKING = {:.4f}\".format(accuracy_score(y_test, y_pred_stacking_pipeline)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hkOsS0DLqvT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(y_test, y_pred_stacking_pipeline))\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred_stacking_pipeline))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFepzWxkwN39"
      },
      "source": [
        "# **Predictions to CSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suFgRdWCfdww"
      },
      "outputs": [],
      "source": [
        "# Predictions\n",
        "# Uncomment whichever model's prediction is desired\n",
        "\n",
        "#RF\n",
        "#y_pred_test = rf_pipeline.predict(test_set)\n",
        "\n",
        "#XGBoost\n",
        "#y_pred_test = xg_pipeline.predict(test_set)\n",
        "\n",
        "#Extra Trees\n",
        "#y_pred_test = xt_pipeline.predict(test_set)\n",
        "\n",
        "#Stacking\n",
        "#y_pred_test = stacking_pipeline.predict(test_set)\n",
        "\n",
        "#Voting\n",
        "y_pred_test = vote_pipeline.predict(test_set)\n",
        "\n",
        "#{'functional': 0, 'functional needs repair': 1, 'non functional': 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "yfHYVkQ80jVC",
        "outputId": "a41b6c7d-009d-4e0e-d20d-6020f53f445b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>status_group</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>59400</th>\n",
              "      <td>50785</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59401</th>\n",
              "      <td>51630</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59402</th>\n",
              "      <td>17168</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59403</th>\n",
              "      <td>45559</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59404</th>\n",
              "      <td>49871</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74245</th>\n",
              "      <td>39307</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74246</th>\n",
              "      <td>18990</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74247</th>\n",
              "      <td>28749</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74248</th>\n",
              "      <td>33492</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74249</th>\n",
              "      <td>68707</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14850 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  status_group\n",
              "59400  50785             2\n",
              "59401  51630             0\n",
              "59402  17168             0\n",
              "59403  45559             2\n",
              "59404  49871             0\n",
              "...      ...           ...\n",
              "74245  39307             2\n",
              "74246  18990             0\n",
              "74247  28749             0\n",
              "74248  33492             0\n",
              "74249  68707             2\n",
              "\n",
              "[14850 rows x 2 columns]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = pd.DataFrame(\n",
        "                            {'id': test.id,\n",
        "                           'status_group': y_pred_test}\n",
        "                         )\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYcl9SuA4DyX"
      },
      "outputs": [],
      "source": [
        "predictions.loc[predictions['status_group'] == 0, 'status_group'] = 'functional'\n",
        "predictions.loc[predictions['status_group'] == 1, 'status_group'] = 'functional needs repair'\n",
        "predictions.loc[predictions['status_group'] == 2, 'status_group'] = 'non functional'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "0sgnCUvi4ebe",
        "outputId": "8677ac7c-d21b-4329-d6f2-74c834a16698"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>status_group</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>59400</th>\n",
              "      <td>50785</td>\n",
              "      <td>non functional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59401</th>\n",
              "      <td>51630</td>\n",
              "      <td>functional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59402</th>\n",
              "      <td>17168</td>\n",
              "      <td>functional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59403</th>\n",
              "      <td>45559</td>\n",
              "      <td>non functional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59404</th>\n",
              "      <td>49871</td>\n",
              "      <td>functional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74245</th>\n",
              "      <td>39307</td>\n",
              "      <td>non functional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74246</th>\n",
              "      <td>18990</td>\n",
              "      <td>functional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74247</th>\n",
              "      <td>28749</td>\n",
              "      <td>functional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74248</th>\n",
              "      <td>33492</td>\n",
              "      <td>functional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74249</th>\n",
              "      <td>68707</td>\n",
              "      <td>non functional</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14850 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id    status_group\n",
              "59400  50785  non functional\n",
              "59401  51630      functional\n",
              "59402  17168      functional\n",
              "59403  45559  non functional\n",
              "59404  49871      functional\n",
              "...      ...             ...\n",
              "74245  39307  non functional\n",
              "74246  18990      functional\n",
              "74247  28749      functional\n",
              "74248  33492      functional\n",
              "74249  68707  non functional\n",
              "\n",
              "[14850 rows x 2 columns]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbJ2H2E1QSeS"
      },
      "outputs": [],
      "source": [
        "# Saving file\n",
        "predictions.to_csv('my_submission.csv', header=True, index=False)\n",
        "\n",
        "#from google.colab import files\n",
        "#files.download('my_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM4agS1GyQBY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vP3UXc_RUqBO",
        "4UWQ1i_hUzOl",
        "zQ4aNgHaV7y-",
        "DgdU3eYfY_qx",
        "5TrlLOYo5Jqn",
        "PswY1M-1Q_cB",
        "QbirwI1wPv01",
        "_Xrv_OlrRIip",
        "DxtcJ-ByDLRc"
      ],
      "name": "Pump_it_up_Optuna Tuned.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}